# 第十七章 并行处理

## Key Points

1. SMP 对称多处理机系统的特征、优点、内部互联组织有哪些形式：总线、多端口内存，优缺点是什么

2. SMP多机系统中，保证 cache 一致性的解决方案，软件以及硬件的。硬件中又有两种，如何处理。分布式目录协议有两种处理方法，写无效和写更新，各自的含义是什么，MESI 属于哪一类
3. MESI 协议的具体内容，M、E、S、I 对应四个状态，各是什么状态？自己位于某个状态时，其他核位于其他状态下读取/写入时，状态如何变化？

4. 集群的定义，轻量级集群的三种应用

5. NUMA 的定义，比起 SMP 和集群有什么优势，典型组织 CC-NUMA 是怎样的，各个处理器如何进行内存访问（通过目录），如何保持cache一致性
6. 注意对比三种并行处理机的优缺点

> 易俊泉学长的原始笔记链接如下：
>
> [chapter17 并行处理](docs/课内笔记/大三上/计算机组成与结构/笔记/易俊泉/chapter17并行处理.md)

## 多处理器组织

对具有并行处理能力的系统进行分类的方法如下：

:o: **单指令单数据（SISD）流**：单一处理器执行单一指令流来操作保存于单一存储器上的数据。单处理器系统属于这一类。

> 流水线和超级流水线都是SISD，超标量不是

![image-20211201112729421](https://raw.githubusercontent.com/yijunquan-afk/img-bed-1/main/img2/1695704557.png)

![image-20211201112918660](https://raw.githubusercontent.com/yijunquan-afk/img-bed-1/main/img2/1695704558.png)

:o: **单指令多数据（SIMD）流**：一条机器指令控制几个处理部件基于锁步方式同时执行，每个处理部件有一个相关的数据存储器，故每条指令在不同的数据组上执行。17.7 节讨论的向量和阵列处理器属于这一类。

> 不同的数据处理相同的操作

![image-20211201112750754](https://raw.githubusercontent.com/yijunquan-afk/img-bed-1/main/img2/1695704559.png)

:o: **多指令单数据（MISD）流**：一系列数据被发送到一组处理器，每个处理器执行不同的指令序列。这种结构从来没有在商业上被实现过。

![image-20211201112822380](https://raw.githubusercontent.com/yijunquan-afk/img-bed-1/main/img2/1695704563.png)

:o: **多指令多数据（MIMD）流**：**一组处理器同时执行不同的指令序列，对不同的数据集进行操作**。SMP、集群系统和NUMA系统都属于这一类。

> 例如超标量
>
> NUMA：紧耦合	松耦合：集群系统

![image-20211201113209167](https://raw.githubusercontent.com/yijunquan-afk/img-bed-1/main/img2/1695704565.png)



![image-20211201112520208](https://raw.githubusercontent.com/yijunquan-afk/img-bed-1/main/img2/1695704560.png)

## 对称多处理器 Symmetric Multiprocessors

SMP 定义为具有以下特征的独立计算机系统：

> :one: 有**两个或更多功能相似（使用相同的架构）的处理器**。（多机）
>
> :two: 这些处理器**共享同一主存**和 I/O设备，以总线或其他内部连接机制互连在一起；这样，主存的存取时间对每个处理器大致都是相同的。
>
> :three: 所有处理器共享对 I/O 设备的访问，或通过同一通道，或通过提供到同一设备路径的不同通道。不管如何，都要求使用通道连接外部设备。
>
> :four: **所有处理器能完成同样的功能**（术语“对称”的由来）。
>
> > 不像有的非对称多处理机系统中，一个处理机专门为其他处理机分配要执行的任务；这里任何处理机都可以执行任务。
>
> :five: 系统被一个**集中式操作系统（OS）**控制，OS提供各处理器及其程序之间的作业级、任务级、文件级和数据元素级的交互。
>
> > 表示了与集群系统之类的松耦合多处理系统的对照。后者所交互操作的物理单位通常是消息或整个文件；而 SMP 中，各个的数据元素能成为一个交互级别，于是处理器间能够有高度的相互协作

SMP的操作系统可以跨越所有处理器来调度进程或线程，具有如下几个优点：

> **性能（performance）**：某些工作部分能够**并行完成**，则具有多个处理器的系统与具有同样类型的单处理器的系统相比，有更高的性能
>
> 相当于并行打并发，肯定是并行赢）
>
> **可用性（availability）**：在一个对称多处理器中，所有处理器都能完成同样的功能，故单个处理器的故障不会造成系统的停机，系统可在性能降低的情况下继续运行。
>
> **增量式增长（incremental growth）**：用户可以通过在系统中添加处理器来提高系统性能。原厂CPU最好。不过，由于多机通信的性能损耗，添加 CPU 数量存在上限
>
> **可扩展（scaling）**好：厂商能提供一个产品范围，它们基于系统中配置的处理器数目不同而有不同的价格和性能特征。

![image-20211201115312397](https://raw.githubusercontent.com/yijunquan-afk/img-bed-1/main/img2/1695704561.png)

多处理机系统对用户是透明的；用户不知道系统中存在多个处理器，只觉得这个机器很快。

SMP 结构上存在多个处理器，通过共享网络实现主存和 I/O 的共享访问。主存一般采用多端口主存实现共享访问。处理器数量一般多于 I/O 设备。

说明：

- 每个处理器是独立的，包括有控制器、ALU、寄存器和Cache
- ﻿通过某种形式的互连机制，每个处理器能访问共享的主存和1/0设备
- ﻿﻿各处理器之间可通过共享存储器相互通信，进行数据或状态信息的交互
- ﻿﻿各处理器之间直接交换信号也是可以的

### 组织

SMP 内部的「互联网络」部分可以如何实现？

#### 分时共享总线

- 最简单的形式

- ﻿﻿结构和接口类似于使用总线互连的单处理器系统

- ﻿﻿具有以下特征：

    ﻿﻿寻址功能：确定数据的来源和去向（一般总线也需要）

  仲裁功能：任何模块应该都能对总线发起控制申请，那必须有相应机制来对不同模块的控制请求进行仲裁，确定总线的使用权，也就是竞争时仲裁（第三章介绍总线时介绍过）

- 具有如下独居的特征：

  分时复用功能：一个模块正在使用总线时，则其他模块的总线需求必须先挂起等到此次操作结束后，才能被响应

  多个处理器和其他模块一样，也是共享总线的，只不过优先级可能比其他模块高。

**优点**

> **简易性**
>
> 每个物理接口以及处理器的寻址、仲裁和分时特性都和单处理器系统相同
>
> **灵活性**
>
> 扩充系统很容易，再挂接新的处理器到总线即可扩展处理器
>
> **可靠性**
>
> 总线上任一 CPU 的故障不会引起整个系统的瘫痪

**缺点**

> 系统性能受限于总线周期时间；本来多个 CPU 可以并行，但因为共享总线的冲突，又只能串行了。
>
> 为了减少总线访问，每个处理器都配置局部 Cache，但又会带来 Cache 数据一致性的问题；为了维护一致性，又需要使用总线
>
> > 所以总线使用越少，总线使用越多（不是
>
> 总体来看使用两级 Cache （局部 Cache）的好处还是要大于维护数据一致性的损失

这是成本最低，最简单的互联方式，适用于小型的 SMP 系统，但是效率也是最低的。

#### 多端口内存

分为内存模块

每个处理器或 I/O 模块直接、独立地访问内存模块，几乎不需要修改处理器或 I/O 模块，但是**端口上需要逻辑去解决冲突**

- ﻿每个端口都必须有解决冲突所需的逻辑，比如﻿优先级排序
- ﻿﻿处理器和 I/O 模块几乎不需要修改就可以使用多端口内存
- ﻿﻿复杂性主要还是在多端口上体现（因为需要解决访问冲突）

![image-20211203101914901](https://raw.githubusercontent.com/yijunquan-afk/img-bed-1/main/img2/1695704566.png)

对于 CPU 在不同端口的优先级的设置一般也不同，否则会使各个 CPU 的地位不均等

**优点**

- 更好的性能

  每个处理器都有通往每个模块的专用路径，在相同 CPU 规模时，它比分时共享总线方式性能好

- 安全性好

  可以将部分内存配置为一个或多个处理器的私有内存，这样安全性更高，不易被其他处理器修改

**缺点**

- 更复杂的控制，会造成内存系统中的额外逻辑（比如优先级逻辑），导致接口复杂且高成本

- 必须使用写直达 Cache 策略进行 Cache 控制，因此处理器之间的通信都通过内存接口，会有潜在冲突的问题

  存储器端口数和类型在设计时就要确定下来，因而系统的规模和配置也就被确定了，很**难扩展**（相对之下，分时共享总线就更好扩展）

- 应使用通过缓存写入策略进行缓存控制，处理器之间的通信仅通过内存进行

### SMP 操作系统设计考虑

SMP 操作系统提供进程并发系统的所有功能和功能，以适应多个处理器，关键的设计问题如下：

> 支持**同时并发进程 **simultaneous concurrent process：允许几个处理器可同时执行同一 OS 代码，需要避免死锁
>
> **调度** scheduling：保证多个处理器负载均衡
>
> **同步** synchronization：由于多个进程是共享存储器或共享 I/O 的，所以必须要有同步机制去保证它们的访问是互斥的，而且对事件也要能排序。可以采用公共变量解决。
>
> **存储管理** memory management：多端口存储器需要解决访问冲突；协调不同处理器共享页的一致性；在页替换时决定合适的操作（比如有的处理器认为这页该置换，其他处理器却需要这页）
>
> **可靠性和容错**reliability and fault tolerant：识别某个处理器的故障，并相应地重构管理表，保证一个 CPU 坏掉不会导致整个系统完蛋

## Cache 一致性与解决方案

Cache 一致性问题 cache coherence：由于主存中同一数据的多个副本被存放在不同 CPU 的局部 Cache 中，如果让各 CPU 自由修改局部 Cache 的数据，就会导致各 Cache 之间及 Cache 和主存数据不一致的问题。

在多机系统中，回写法（平时只写 Cache 不写主存，替换出 Cache 内容时，将 Cache 内容写回内存）肯定会导致数据的不一致，因为存在多个局部的 Cache

在多机系统中，写直达（每次写时既写入 Cache 也写入内存）也会出现数据不一致的问题，除非其他 Cache 监听主存的访问情况，否则其他 Cache 无法得到写后的新主存数据。

### 解决方案

#### 软件解决方案

依赖于编译程序和操作系统

在编译程序编译时完成代码分析，确定哪些数据对于 Cache 来说是不安全的

接着编译程序对这些数据项进行标注，避免把这些不安全数据放到局部 Cache 中去，规定这些数据只能在主存中进行存取

然后操作系统或硬件机制负责防止这些数据被用于 Cache

**优点**

> 维护一致性开销由运行时间变成编译时间（提前决定内容是否可以进入 Cache）
>
> 设计复杂性由硬件转移到软件

**缺点**

> 由于编译器判断通常比较保守，Cache 利用率还是会下降
>
> 对于编写编译器和操作系统的程序员要求比较高

#### 硬件解决方案（Cache 一致性协议）

- 在程序运行时动态识别潜在的 Cache 不一致问题
- 由于可以在运行时识别，不用保守判断，Cache 的使用更有效
- ﻿﻿对操作系统程序员和编译器程序员透明，减轻了软件开发的负担

实现：采用一个目录来记录共享数据信息，有 Cache 修改需求的时候要先报送给目录，并发广播，收到其他 Cache 确认后才可以修改。

目录的实现形式主要包含两种：

**集中式目录协议（目录协议）**

> 基本思想就是不管有多少个 CPU 和局部 Cache，系统都只维护一个目录，存放到专门的 Cache 或者是主存里去，这个目录记录并维护共享数据的副本存放在哪个局部 Cache 中
>
> 一般由集中式 Cache 控制器进行维护和发送。

读：

- 当一个局部 Cache 想要读一个主存数据时或者想读其它 Cache 的数据时，必须先报送到集中式
    Cache 控制器，集中式 Cache 控制器将检查这个请求并发出允许命令，同时将这个数据的记录变为共享的，然后局部 Cache 和主存之间才可以进行数据传送
- ﻿﻿总之，任何一个能影响主存块的全局状态的动作必须报告给集中式 Cache 控制器，集中式 Cache 控制器通过更新目录来记录相应的信息。

写：

当一个处理器要写它的局部 Cache 了，也不可以直接写，先要向集中式 Cache控制器报送它这个请求，然后控制器就会先把这个数据行设置成该处理器的专有数据，并将该行在主存中状态设置为无效，然后检查目录，看看要写的这一行是否还在其他的局部 Cache 中。

- 如果这个行没有出现在其他 Cache 中，那么就让这个处理器去直接修改它的局部 Cache 数据
- ﻿﻿如果这个行还出现在其他局部 Cache 中，那集中式控制要先通知所有局部 Cache 中持有这个行的其他处理器，让它们把自己的局部 Cache 的这个数据行变为无效，然后发回一个确认信号给集中式 Cache 控制器

直到收到所有这些处理器的确认信号，集中式 Cache 控制器才允许原来那个发请求的处理器修改自己的 Cache 数据。

此后如果还有一个其他的处理器想要读主存中这行数据时，它先会发现 Cache 中这行数据无效，接着它就会发一个未命中信号给控制器，控制器就会向目前持有该行有效数据的处理器发一个命令，让它把这个数据行从局部 Cache 写回主存，然后其他处理器再通过主存读该行数据。

此时这个数据行又被多个处理器所共享了，又成为共享数据。

缺点：

- 这个集中式 Cache 控制器负责的事务非常多了，每次的数据修改都要报送给它，通信负担重
- ﻿﻿各个局部 Cache 的控制器和集中式 Cache 控制器的交互信息很多，通信开销很大
- ﻿﻿集中式 Cache 控制器有可能就成为系统的瓶颈

但在多总线结构的 SMP 系统中，这个目录协议还是很有效的

**分布式目录协议（监听协议）**

其思想就是将维护 Cache 一致性的任务分散到各个处理器的 Cache 控制器，不建立统一的集中 Cache 控制器。

这些局部 Cache 必须能够识别它里面的数据行是否共享的

当某 CPU 想要对自己 Cache 中的共享数据行进行修改时，通过广播向其他 Cache 和主存通知；其他 Cache 控制器需要监听到消息并且进行对应操作。具体操作根据协议不同而不同。

此协议不允许两个 Cache 直接通信，Cache 数据交互必须要通过主存来完成，也不允许某个
 CPU 对自己 Cache 中无效数据直接进行改写（需要先发广播）

因为这两种情况都会造成其它Cache或主存不能对数据进行监控。

监听协议特点：

- 适合基于总线的多机系统，因内总线能为广播和监听提供简单的方式
- ﻿增加总线传输开销可能抵消掉采用局部Cache的收益
- ﻿﻿具体可分为两类
  - ﻿基于写无效的监听协议
  - ﻿基于写更新的监听协议

**基于写无效的监听协议**

类似于回写策略，适合数据更新频繁的情况。

> 当某个 CPU 要对自己 Cache 的数据进行修改的时，通知其他还存有此数据的 Cache 将此行变为无效，并把这个行变为自己 Cache 所专有。
>
> 这样，之后其他处理器要访问此行时，就必须去主存读取修改后的内容
>
> ﻿一旦某 Cache 行变为专有，拥有者处理器就可进行本地写操作，直到其他处理器提出读取此数据行的要求。
>
> 需要使用四个标记位标记每个 Cache 行的状态：修改态M、专有态E共享态S以及无效态I，因此此协议也被称为：<font color="red">**MESI协议**</font>

**基于写更新的监听协议**

类似写直达策略，不适合数据更新频繁的情况。

> 当一个处理器想要修改某个共享行，它也先要把这个具体想修改的行广播给其他 Cache，然后其他包含此行的 Cache 跟着**发起修改 CPU 同时进行自己数据的修改**
>
> Cache 间通信数据大小大于写无效方法，因此修改话费的时间更长

### MESI 协议

MESI 协议就是 Cache 一致性协议中的监听协议，「基于写无效的监听协议」。其每个 Cache 需要存储的内容包含四个状态（共计二位二进制数）

- 修改态M
  - ﻿﻿此行已被修改，仅在本地 Cache 可用

- 专有态E
  - ﻿﻿此行只在主存和本地 Cache 可用，任何其他 Cache
     中都为不可用
- 共享态S
  - ﻿﻿此行在主存及另外的几个 Cache 中都是可用的
- 无效态I
  - ﻿﻿此行在本地 Cache 中为不可用的，用时必须从主存要可用数据

Cache 只可能处于这四种状态中的一种。

MESI 协议通过有限状态机描述 Cache 状态的转换：

![image-20241219112159757](https://telegraph-image-5ms.pages.dev/file/BQACAgUAAyEGAASIfjD1AANDZ6F8OhDRiQFSqJ2Kw19xh46RAYUAAqsSAAK0lBBVLiRLrPmPex02BA.png)

左图：发起动作的 Cache 的状态；右图：接受通知的 Cache 的状态。主存不分四种状态。

左图（发起者）的状态变化如下：

读无效态的数据：读未命中，需要访问主存，此时存在多种状态

- 此数据在其他 CPU 中为共享态：直接从主存读取数据，读后自身状态也变为共享态
- 此数据在其他 CPU 中为专有态：从主存读取数据，通知其他 CPU 变为共享态，自身状态也变为共享态
- 此数据在其他 CPU 中为修改态，主存数据无效：要求其他 CPU 将其 Cache 写入主存（主存变为有效），自己再读取主存；完成后自己和其他 CPU 都变为共享态。
- 此数据仅在主存中存在：直接读取数据，读后自身状态变为专有态。

不允许直接写无效态的数据；必须先读取数据，再修改读取来的数据，即带有修改目的的读。写完后，Cache 状态变为修改态（新的内容只有自己 Cache 存在），其他 CPU 的 Cache 状态会变为无效态。

读共享态的数据：读命中，直接读取，不会导致自己和其他 CPU 的状态变化

写共享态数据：写命中，写后需要通知其他 CPU 将其 Cache 状态变为无效，自己的状态变为修改态

读专有态数据：读命中，直接读取，不会导致自己和其他 CPU 的状态变化

写专有态数据：写命中，导致自身状态变为修改态（主存内容无效），其他 CPU 本来就是无效

读修改态/写修改态数据：全部不会引发状态改变（因为只有自己的 Cache 存在此数据，我爱怎么改就怎么改）

右图（监听者）的状态变化如下：

本身处于修改态，其他人读取我的数据时：此时数据先写入主存，再让其他 CPU 利用主存读取，之后自己的状态成为共享态。

本身处于修改态，其他人写入数据时：其他人应当为无效态，因此和读一样，数据先写入主存，再让其他 CPU 利用主存读取，之后自己的状态成为无效态，其他人修改后，其状态成为修改态。

本身处于专有态，其他人读取时：自身状态变为共享态

本身处于专有态，其他人写入时：自身状态变为无效态，发起者修改此数据。

本身处于共享态，其他人读取之后，自身状态不需要变化；

本身处于共享态，其他人写入时：自身状态变为无效态，发起者修改此数据。

本身处于无效态时，不管其他人进行读写，都和我没有关系，本身的状态不会变化。



需要注意的是这两个图不是割裂开来的，一个 Cache 行必须依据这两个图进行状态迁移，具体看是本地 CPU 要读写，还是监听到其它的 CPU 要读写

整个协议都是用硬件实现的



多级 Cache 的协议：一般采用写直达策略。

- 写 L1 Cache 的同时，也写入 L2 Cache，但不要求写入主存

- L2 Cache 和主存采用 MESI 协议

- 因此，也称为扩展 MESI 协议

## 集群

集群（Cluster）系统是由很多的独立计算机通过互联网或者是其他专用网络连接而成，它们之间可以协同工作，并对外表现为一个统一的集中的计算机资源，来供并行任务处理使用。就像操作系统中提到的「分布式系统」。

**物理上有若干台机器，逻辑上只有一台**

其中每一台计算机就被称为一个结点 node

>  这里一个结点就是一个完整的系统，拥有本地磁盘以及操作系统，完全可以作为一个单独的计算机资源，脱离集群供用户使用。
>
> 这是和 SMP 的最大区别：SMP 中的任何 CPU 在离开集群后都无法工作。

集群的具体性能由构成机群的结点性能来决定，**不要求同构**

> **每个结点上也可以安装不同操作系统，只不过在每个结点上面还要再安装集群的操作系统**
>
> SMP 也可以用于构成集群

> :label: 为了性能，通常每一个node会采用smp
>相当于一台计算机：维护成本高
> 需要有Cluster操作系统

![image-20211203105027919](https://raw.githubusercontent.com/yijunquan-afk/img-bed-1/main/img2/1695704567.png)![image-20211203105145265](https://raw.githubusercontent.com/yijunquan-afk/img-bed-1/main/img2/1695704568.png)

### 优点

**:one: 绝对的可扩展性 absolute scalability**：一个集群可以有数十台计算机，每台计算机可能是一个多处理器，而且扩展计算机数量及其简单

:two: **增量可扩展性 increment scalability**：可以逐步扩展集群

:three: **高可用性（容错性高） high availability**：具有很好的容错性

:four: **卓越的性价比 superior price/performance**：使用现有的机器构建多级系统，所需的成本很少

### 缺点

维护工作量大，维护费用较高

为了保证系统的高性能，同时减小维护成本，很多集群采用 SMP 作为其节点（实现相同的处理器量所需要的节点数量最少）

### 轻量级集群 light weight clusters

用于轻量级应用。

**被动式备用 passive standby**：

- ﻿属于热备份
- ﻿两台服务器同时开机，但其中一台机器不承担用户负载，只是开了机，系统开销是零
- ﻿活动的主服务器周期地发送“心跳”消息给备用服务器
- ﻿﻿如果主服务器坏了（心跳信号断掉），另一服务器马上接管服务
- ﻿﻿接管只能是功能上的接收，**没有接管数据**。如果用户提交服务到了一半，主服务器挂了，那么接管后用户还需要再次申请

**主动式辅助**：两台机器都要承担用户负载。分为三种：

> 三种主动式辅助都能实现接管磁盘数据

独立的服务器（对等式服务器）：

- ﻿每个结点都有自己磁盘，这个磁盘也不共享
- ﻿让两台机器同时工作并进行负载均衡，同时还要进行数据备份
- ﻿当一台机器坏的时候，连它正在处理的工作，其他机器都能完全接管（因为可以读取其他机器的备份）
- 由于需要不断的备份，﻿通信开销比较大

连接磁盘的服务器：

- ﻿每个结点都有各自磁盘，但除了结点间有通信链路，各个磁盘间也存在线路连接
- ﻿﻿如果一个服务器发生故障，它的磁盘也能被其他服务器接管
- 不需要不断进行数据备份

共享磁盘的服务器

- ﻿每个结点的磁盘都是共享的，因此任务接管就更完整，速度也快
- ﻿这种配置在大型网站应用最多
- ﻿共享磁盘一定要有锁定机制，保证某个数据一次只能被一个结点写入

### Cluster 操作系统设计需要考虑的问题

**故障管理**：操作系统需要发现并且识别故障的节点

**负载均衡**：要把任务合理的分配给多个节点；进一步，需要同时移交数据，让用户不用再次执行任务。在新的节点加入时，需要实现将任务平均分配到新的节点上，保证负载均衡

**并行化计算**：保证任务可以并行化的执行

## Non-uniform Memory Access System 非均匀存储访问系统

### 为什么需要 NUMA

对于 SMP 系统，随着处理器数量的增加，总线可能会成为性能瓶颈；而且还需要维护 Cache 的一致性，因此系统负担较大。而且，由于通信消耗，增加处理器数量对性能的增加存在上限

使用集群时，每个节点都有自己的专用主内存，应用程序看不到较大的全局内存，无法运行极大的应用程序。

NUMA 可以补偿上述限制

![image-20211203111457407](https://raw.githubusercontent.com/yijunquan-afk/img-bed-1/main/img2/1695704569.png)

对于NUMA中的每一个CPU，可以看到统一的大内存

### 概念

**多个独立计算机通过网络组织成一个计算能力很强的计算机，这些计算机是<mark>共享内存</mark>的，网络上的每一个计算机都是一个结点**

其中每个处理器都有局部的存储器系统，但所有局部存储器会构成共享的全局存储器

这个存储器逻辑上是统一编址的，其中的每一个结点都可以访问所有的存储器空间，但是对存储器不同地址范围，其物理位置肯定不同，所以真正的存取时间会有所不同。

> 即大内存是通过各个计算机共享内存实现的，而不是通过买一块超级大的内存装上去

由于对本地存储器的访问比位于其他结点的存储器的访问速度要快，所以这种共享存储器多机系统称为非均匀访问系统 NUMA

### 典型组织 CC-NUMA

典型的是带有 Cache 一致维护的 NUMA（CC-NUMA）——在各处理器的 Cache 之间有维护 Cache 一致性机制的 NUMA 系统

没有 Cache 一致性维护的 NUMA 系统或多或少等同于集群系统

![image-20211227174345953](https://raw.githubusercontent.com/yijunquan-afk/img-bed-1/main/img2/1695704562.png)

> 多个独立的结点通过某种通信网络互连
>
> 为了适用于高性能计算，每个结点都是 SMP 组织
>
> 每个节点都需要安装相同的操作系统（为了共享内存），这和集群的要求不同

每个节点存在**目录**，记录自身的内存充当了整体大内存的哪一部分。

> 每个处理器访问数据时先找自己的 1 级 Cache，如果没有再找局部 2 级 Cache，两个都没有才需要访存，访存时就可能要跨网络访问其他结点，访问其他结点主存

访问内存的方法：

- 本结点通过**目录**查出本地某个处理器需要的数据在哪个结点
- ﻿﻿如果不在本地结点，本地结点目录就把请求某个数据的任务封装成消息发送到网络上对应位置的结点
- ﻿﻿对应位置结点的**目录**捕获到该消息并解析消息，然后访问自己的内存，把所需数据取出再打包为消息发给原来发出请求的结点目录

 这些操作对 CPU 和 Cache 都是透明的，访问内存由目录代为执行，不需要 OS 关注。

### 优缺点 Pros and Cons

**优点**

> 在主要应用软件不变的情况下，理想时 NUMA 的性能可能达到 SMP 的若干倍
>
> ﻿特别是网络通信不是特别多时，也就是访问集中在本地结点内存时，性能会比较好

操作系统的设计普遍采用页移植的机制：

如果某个节点的进程需要的数据总在远端的节点，就主动把进程转移到远端执行；执行完成后，把执行结果返回给原先的节点，可以减少网络通信。

**缺点**

> NUMA 的操作系统成本很高且很难编写（就像 SMP 的编译程序，很难做）
>
> 性能和其具体实现有关