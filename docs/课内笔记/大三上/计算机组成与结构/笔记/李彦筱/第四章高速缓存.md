# 第四章 高速缓存

> 易俊泉学长的原始笔记链接如下：
>
> [chapter04 cache 存储器](docs/课内笔记/大三上/计算机组成与结构/笔记/易俊泉/chapter04cache存储器.md)

## KEY POINTS

1. 存储器系统的特性有哪些，其中一个重要的就是数据的存取方式。存在哪些数据存取方式？哪些存储器采用哪些存储方式，速度如何？

3. 存储器的分层结构

4. 局部性原理：时间局部性与空间局部性
5. <mark>Cache 是重头戏，可以出大题</mark>

6. Cache 的概念、目的、对于程序员是否透明、写入方式是什么？Cache行和块的关系，怎么对应，典型的 Cache 组织方式

7. Cache的设计要素有哪些，容量、映射方法、替换算法、写策略、行大小、Cache 数目选择。重点为 Cache-主存映射方法和替换算法。

8. <font color="red">映射功能：直接映射、关联映射和组关联映射</font>

9. Cache 数目，多级 Cache 的级数、统一和分立 Cache 各自的特点

计算机的存储器被组织成层次结构，最顶层(最接近处理器)是处理器内的寄存器，接下来是一级或多级的高速缓存 Cache，再往下走是主存，通常由动态随机存取存储器 DRAM 构成，以上这些都是系统内部的存储器。在其下，存在一层外部存储器，通常指硬盘，再往下走是可装卸的存储设备。

## 计算机存储系统概述

### 存储系统的特性

存储系统最重要的一些特性如下：

![image-20211031201134319](https://raw.githubusercontent.com/yijunquan-afk/img-bed-1/main/img2/1695704116.png)

位置：存储器在计算机的内部还是外部，联机（一直连接）还是脱机（磁带、光盘等不常连接）内部存储器通常等于主存。

> 本章讲解内部存储器中的 cache（高速缓存），下一章讲解主存，6-7 章讲解外部存储器。

容量：容量一般用字或者字节表示。

**字**:存储器组织的“自然”单元。字长通常与一个整数的数据位数和指令长度相等，也一般等于传输宽度（即进出存储器的电线条数），但也有很多例外。字长一般是处理器能够运算的数的最大长度。

**可寻址单元**：很多系统中，允许同时以字和字节为单位进行寻址。在任何情况下，地址位长度 A 和可寻址的单元数 N 之间的关系为:  $2^A=N$。

> 比如如果地址位长为 4 位，那么最多就能使用 16 个编号，因此最多可寻址 16 个单元

一般情况下，对于可以同时使用字节和字寻址的系统，字节地址和字地址存在关联：**字节地址的高位部分就是此单元的字地址**。具体取多高位呢？记住，**去掉的位数 = $log_2(一个字包含多少个字节)$**。比如，在下面的图中，一个字包含两个字节（即一个字 = 2 字节 = 16 位），因此去掉最低的 $log_2(2)=1$ 位之后就可以得到字地址。

![图片 1](https://telegraph-image-5ms.pages.dev/file/BQACAgUAAyEGAASIfjD1AAMtZ6DY7TV_MdE0jcMCuIWdrgVZP0MAAgMYAAK0lAhVmHAuv7IC3SY2BA.png)

> 比如，字节地址 1100（12）处，去掉最后一位，得到 110（6）就是它的字地址。

![图片 2](https://telegraph-image-5ms.pages.dev/file/BQACAgUAAyEGAASIfjD1AAMuZ6DY_Qa2WNj-FZ5Xs-_HL5waYPsAAgQYAAK0lAhVUXYQEKdg2UM2BA.png)

> 在这张图片中，一个字=4个字节（16位），因此对于字节地址 1100（12），我们去掉后两位 00，得到 11（3）就是其字节地址。

从字地址得到字节地址就是一个相反的过程：将字地址左移几位（位数=$log_2(一个字包含多少个字节)$），然后加上这个字节相对于其所在的字开始位置的偏移量。

**传输单元**：对于主存储器，这是指每次读出或写入存储器的位数。传输单元一般等于系统采用的地址形式，比如按字寻址时，最少传输一个字的数据；按字节寻址，则最少传输一个字节的数据。对于外部存储器，数据的传送经常是以比一个字大得多的单元来传送，这就是所谓的**块**。

> 顺序访问、直接访问、随机访问也是三个常见的关键词，具体含义请查看下一节喵

从用户的观点来看，存储器两种最重要的特性是**容量和性能（Performance）**,通常需要3种性能参数:

**访问时间(延迟)**：对于随机存取存储器，这是执行一次读或写操作的时间，即从地址进入总线的时刻到数据已经被存储或使用所花的时间。而对于非随机存取器，访问时间是把读写结构定位到所需要的存储位置所花费的时间。（查找所花费的时间），因为读取时间相比查找时间来说非常短

**存储器周期时间**：这个概念主要用于随机存取存储器，它是**存取时间加上下一次存取之前所需要的附加时间**。这里附加时间用于瞬变的信号消失或数据破坏性读后的再生。需要注意，**存储周期时间是与系统总线有关，而不是与处理器相关。**

> 通俗地说，DRAM 读取后，电荷就消失了，需要再充上电才能正常的存取，无法立刻继续存取数据，因此有这么一个概念
>
> 注意周期时间不只是再生时间，还需要加上访问时间。

**传输率**：这是数据传入或传出存储单元的速率。对于随机存取存储器，它等于“1/周期时间”。而对于<font color="red">非随机存取存储器，有下列关系</font>:
$$
T_N=T_A+\frac{n}{R}
$$
其中：$T_N$：读或写N位的平均时间

​	$T_A$：平均读取时间

​	$n$：读取内容的位数

​	$R$：传输率，单位是b/s（位/秒）（这个一般算不出来，会直接告诉你）

即非随机存取存储器读取内容花费的时间包含了两部分：顺序查找数据时间（Ta，平均读取时间）和读取时间（读取内容位数/传输率）

**物理特性**：存储器可以按照其物理特征分为易失和非易失的。易失指的是断电之后数据就丢失了，比如主存；非易失是指断电后数据不会丢失，比如磁盘、ROM

还可以分为可擦除和不可擦除的。可擦除是指存储器可以读和写，比如 RAM 和磁盘；不可擦除是指存储器一旦写入后，只能读取，不能再写入，比如 ROM 和大部分光盘。

> 不可擦除的存储器一定是非易失的，否则你写入数据并断电后，就会得到一块没数据可读、还没法写的废物砖头（

#### 存储器的存取方式

存取方式包括以下四类

1. Sequential 顺序存取：必须按照特定的线性顺序来访问。

   - 从头开始，按顺序通读，无法随机访问

   - 传输单元较大

   - 访问时间取决于数据的位置和之前的位置

   常见设备：磁带

2. Direct 直接存取：块之间可以随机访问，但是每个块内的内容都只能顺序存取

   - 每个块（一大块存储空间）有唯一的地址

   - 访问是通过跳转到附近加上顺序搜索

   - 访问时间取决于位置和之前的位置

   例如机械硬盘（磁盘），光盘（硬盘可以在磁道之间任意跳转，但是磁道内必须顺序访问，即等磁头转到你要访问的位置）、NAND（闪存，即 SSD）

3. Random 随机存取：

   - 通过每一个地址准确地识别位置
   - 访问时间与位置或之前的访问无关

   如 DRAM（主存），通过译码的形式定位，访问任何位置的时间都一样。

4. Associative 关联存取（随机存取的特例）
   - **是一种随机存取**
   - 数据是通过与地址中所有**字的部分内容**进行比较来定位的（即有部分地址就可以部分定位）
   - 访问时间与位置或之前的访问无关

   例如缓存（Cache）

### 存储器层次结构

对于存储器，我们一般需要关心三个问题：**容量**，**速度**和**价格**。容量越大越好，速度越快越好，价格越低越好。很可惜，目前没有存储器能完美的满足三个要求。

存取时间越短，平均每位的花费就越大；存储容量越大，平均每位的花费就越小；存储容量越大，存取时间就越长。三者是相互矛盾的。

![image-20211225135123451](https://raw.githubusercontent.com/yijunquan-afk/img-bed-1/main/img2/1695704117.png)

以上的存储器结构金字塔中，上层是速度快，但容量小且每位很贵的存储器；下层是速度慢，容量大而且每位便宜的存储器。数据存放的原则是：使用频率越低，应当往下层放；使用频率高，应当往上层放。

对于以上的层次结构，随着层次的下降：每位价格下降、容量增大、存取时间变长、处理器访问存储器的频率降低

### 局部性原理

**局部性原理**：在程序执行过程中，CPU 访问指令或数据时，往往是以块为单位的。

主要原因：

- 程序通常包含许多循环和子程序——即程序大概率会顺序引用存储器中的大块区域；
- 表格和数组涉及访问聚集数据集——重复引用
- 程序或数据总是按顺序存放（冯诺依曼结构规定的），需要访问的指令或数据通常与当前指令或数据靠近。

**90/10 法则**：

通常情况下，10% 的代码在执行时耗费了 90% 的时间

第一章中的定律告诉我们，要优化代码的运行速度，就要优化代码中耗时最长的一部分的运行速度。这个法则告诉我们，耗费时间的代码往往占比不大，我们应当集中优化这些代码。

**两种局部性**：

时间局部性：如果一段代码被访问了，它在一段时间内大概率会被再次访问。

> 这是因为上面说到的重复引用/程序循环的问题

空间局部性：如果一块内存被访问了，它附近的内存块大概率会在一会后被访问。

> 这是因为程序和数据大概率都是顺序存储的，因此取一部分数据时，大概率一会要取其附近的数据

### 缓存命中率

命中率（h）：$\dfrac{命中缓存的次数}{访问主存的次数}$

平均访问时间计算：
$$
T = hT_1+(1-h)(T_1+T_2)\\=T_1+(1-h)T_2
$$
$T_1$：访问缓存所需的时间 $T_2$：访问主存所用时间 h: 命中率

这段公式大概的意思是：命中缓存时，花费 $T_1$ 时间（加号前部分），未命中缓存时，花费 $T_1+T_2$ 的时间。这是因为 CPU 需要先访问缓存，才能确定缓存是否命中，因此耗时不是 $T_2$ 而是 $T_1+T_2$。

我们假设命中率为 0 或者 1，看看访问时间在这两种极端情况下是什么样的。

当 h=0 时，即永远无法命中缓存，T = $T_2$，这意味着加入缓存在最差情况下不会让访问速度变慢，最多低到访问内存的程度。

当 h=1 时，所有访问请求都可以命中缓存，此时 T=$T_1$，说明加入缓存在最好情况下可以让访问速度达到缓存的读取速度，这是非常大的提升。

现代计算机的缓存命中率大多在 95% 以上。缓存为计算机带来了很大的加速。

**主存和内存**

**主存 != 内存**。主存就是 Memory，二级的存储器；内存严格意义上指的是「内存储器」，主板上的所有存储器，包含主存、cache 和 CPU 中的寄存器。

## Cache 存储器原理

cache 存储器的目的是平衡快速存储器的昂贵价格和慢速存储器的大容量。cache 的访问速度快于主存，但单价更高，容量更小。它的位置在主存和 CPU 的核之间。

> cache 是 SRAM 制作的，集成度相比 DRAM 更低，速度更快。

cache 中存放了主存部分内容的副本。当 CPU 试图访问主存中的某个字时，首先检查这个字是否在 cache 中，如果是，则把这个字传送给 CPU；如果不是，则将**主存中包含这个字固定大小的块**读入 cache 中，然后再传送该字给 CPU（不用给 CPU 传输整个块）

> 为什么要传递一整个块而不是一个字呢？难道传递更多东西不花时间吗？
>
> 因为上面提到的局部性原理，当把某块数据存入 cache，以满足某次存储器的访问时，CPU 将来还很有可能访问同一存储位置或该数据块中的其他字。
> ![image-20211031203640712](https://raw.githubusercontent.com/yijunquan-afk/img-bed-1/main/img2/1695704128.png)

> CPU 和 cache 传输时，单位为字；cache 从主存读取时，一次读取一个块的内容。

主存储器有多达 $2^n$ 个可寻址的字组成，每一个字都有唯一的 n 位地址，我们将主存看成许多定长的块，每个块有 K个字，那么主存总块数为$M=2^n/K$

为了方便把主存中的块搬到 cache 中，cache 也被分为很多块，每块大小等于主存中块的大小，每一块都称为**行**，和主存一样，每行都是 K 个字，还有几位标记以及控制位。行的长度，不含标记和控制位，称为行大小。

cache 行的数量远远小于主存的块的数目，所以，单个行不可能永远的被某个块专用，需要一个标记位 tag 指示当前行存储了主存的哪个块，tag 的内容通常是主存储器地址的一部分。

![](https://raw.githubusercontent.com/yijunquan-afk/img-bed-1/main/img2/1695704119.png)



cache 读操作：读取主存内容时，cache 一同读取这个字所在的整个块，放到内存的一个行中。

> 用个不太恰当的比喻讲，这就是「一人得道，鸡犬升天」，一个字被 CPU 访问，其所在的整个块都会被复制到 cache 来。

![image-20211031204815738](https://raw.githubusercontent.com/yijunquan-afk/img-bed-1/main/img2/1695704129.png)



上面提到，cache 行数远小于内存块数，因此不可能长期存储内存的某个块。当 cache 满后，它需要删除一个行中的内容，再读取一个新的块进来。

### Cache 块的编号

多个字被编为一个块，而**块开始的字地址的高位**就是这个块的编号。

> 块和字的编号关系就像字和字节的编号关系一样

块号 = 字编号去掉后面的 i 位，其中 $i = log_2(一个块包含多少字)$。

比如，下面的图片中，一个块包含两个字，那么块号 = 字编号去掉最后的 1 位，即 0110 的字号对应了 011 的块号。

![图片 3](https://telegraph-image-5ms.pages.dev/file/BQACAgUAAyEGAASIfjD1AAMvZ6DZGpf0k9IW-R4v8SB36pW4qLkAAgUYAAK0lAhVoKrGbmLYCbg2BA.png)

### Cache 的典型组织方式

![image-20240925105026428](https://telegraph-image-5ms.pages.dev/file/BQACAgUAAyEGAASIfjD1AAMwZ6DZHVCpmF7w4TjKj7gVhob2naQAAgYYAAK0lAhVaQTeLUALmRk2BA.png) 

cache 的典型设计方式如上图所示：

- 缓存命中时，CPU 直接从 cache 中取内容，不需要使用系统总线
- 缓存未命中时，CPU 打开两个缓冲区，利用系统总线从主存中取数据。数据中，需要的字被直接交给 CPU，块内其他部分被读入缓存。

## Cache 设计

cache 设计的主要要素如下：

容量（Size）、映射机制（Mapping Function），替换算法（Replacement Algorithm）、写策略（Write Policy）、块大小（Block Size）、Cache 数目（Number of Caches）、Cache 地址（address of Cache，不关注）

> 映射机制：新的块进入 cache 时，放在哪个行中
>
> 替换算法：新的块进入 cache 后，如果 cache 已满，应该删掉哪一行
>
> 块大小：Cache 的总容量固定情况下，每一块（一行）应该设计多大？



![image-20211008101540485](https://raw.githubusercontent.com/yijunquan-afk/img-bed-1/main/img2/1695704120.png)

### cache 容量的选择

若选择小内存：便宜；命中率低，因此提速作用有限

若选择大内存：命中率高，整个存储系统的平均存取时间接近 cache 的存取时间；但是花费高；门电路多，速度低于小内存的 cache；而且会占用更多的 CPU 空间

要做好权衡，没有最好的选择，cache 在 1kb~512kb 是比较好的

### cache 映射机制(超级大考点)

cache 需要映射机制，是因为 cache 的大小远小于主存大小，需要定一个「规矩」，规定主存中哪个位置的块映射到 cache 的哪一行。用于解决「往哪放、这行是谁」的问题。

映射机制和替换算法是用**硬件**实现的（即门的互联方式决定了其运行方式）

有三种典型的映射机制：直接映射（Direct Mapping）、全关联映射（Associative Mapping）和组关联映射（Set Associative Mapping）。



这三种映射方法都包含以下条件：

> cache 能存储 64KB。
>
> 数据在主存和 cache 之间以每块 4 字节大小传输。这意味着 cache 被组织成 $16K =2^{14}$ 行,每行 4 字节。
>
> 主存容量为 16MB, 每个字节直接由 24 位的地址（$2^24=16M$） 寻址。因此，为了实现映射，我们把主存看成是由 4M 个块组成，每块 4 字节。

### 直接映射

主存中的块 j 和 cache 中的行 i 有如下直接映射关系：$i=j\ mode\ m$

其中：m 为 cache 的行数；j 为主存某个块的块号；i 为这个块进入 cache 时，存放在 cache 里的行号。

![image-20240925112031530](https://telegraph-image-5ms.pages.dev/file/BQACAgUAAyEGAASIfjD1AAMxZ6DZHzqzoUJtF-BvYh8jV2FsXjcAAgcYAAK0lAhVX9T8bbviLC42BA.png)

可以看出，比如 cache 8 行、主存 16 行，那么块号 0010 和 1010（2 和 10）都会被分到行号 10（2）

#### 块地址和行地址的关系

在直接映射中，**一个块在 cache 中对应行的地址= 块的低位**。具体是取低几位呢？这取决于 cache 的行数。

**主存中低若干位相同的块会被分到 cache 中的相同块中**。

到底取低多少位呢？n = $log_2(m)$，其中 m 为 cache 的行数。比如：

![image-20240925112535180](https://telegraph-image-5ms.pages.dev/file/BQACAgUAAyEGAASIfjD1AAMyZ6DZInLyIjlTloLMXnANlpSJy14AAggYAAK0lAhVGaudEm24bnI2BA.png)

比如，在这个例子中，cache 有 4 行，主存有 16 个块，那么就取块号地址的低 $log_2(4)=2$ 位作为行号地址。

> 为什么取后几位就可以呢？本质上是因为这种方法的公式。取模运算等价于取这个数的后几位，因此 i = j mod m 本质上就是说把 j 取后 $log_2(m)$ 行，得到的就是 i。

不同的块可能存在同一行中，那怎么区分这行存的是哪个块呢？

**主存块号去掉行号地址后，剩余的高位不同，可用于区分一行存的是什么块**。

比如，上面的例子中，块号 0001（1）和块号 0101（5）共享行号 01（1）这一行；不过，它们的前两位分别是 00 和 01，这样就可以区分第 1 行存的是块号 0001 还是块号 0101 了。

#### 直接映射的实现

> 写在前面：主存地址和 cache 地址是「通用」的：CPU 想要取数据时，传递的永远是主存地址，cache 解析主存地址，检查自己对应行的位置。不会出现「CPU 想访问缓存就给个 cache 地址，想访问内存就给个主存地址」这种情况

存储器的地址会被分为三部分：（假设每个地址精确到字的级别，**注意不是精确到块**）

![image-20211008104318843](https://raw.githubusercontent.com/yijunquan-afk/img-bed-1/main/img2/1695704132.png)

我们可以通过解析这个主存地址，得到这块内容在 cache 中应该在哪个行，进而去 cache 寻找；如果没找到，可以去主存里根据地址找到这个块，搬回 cache 里。

低位是字内容，表示当前字是块里的第几个字。根据一开始的结论，去掉后 2 位即可得到块号。

中间的位标识 cache 的行，这 14 位表示了这个地址的内容在 cache 中应该在哪一行。

高位标识 cache 的这一行存了哪一个块。通过中间的位，我们已经找到 cache 中对应行了。现在只需要对比 tag 和 cache 中存储内容的 tag 是否相同，就知道 cache 这一行是否存了我想要的块了。

tag 域位数是用 block 域位数减去 line 域位数算出来的，不是直接分配

> Cache 中存储一行时，会额外存储一个 tag（就是上面提到的主存地址的高 8 位），以便指示这行到底对应主存的哪个块。

![image-20240925115227422](https://telegraph-image-5ms.pages.dev/file/BQACAgUAAyEGAASIfjD1AAMzZ6DZJFc2_Hg-GNnIJG5GyJlHScYAAgkYAAK0lAhVHQF9KgABlN-hNgQ.png)

<font color="red">**总结如下**</font>

s：地址中块号的长度

w：地址中字号部分的长度。比如一个块有 4 个字，它的 w 就是 2。

> 地址长度=（s+w）位
>
> 可寻址的单元数=$2^{s+w}$个字或字节
>
> 块大小=行大小=$2^{w}$个字或字节
>
> 主存的块数=$2^{s}$
>
> cache 的行数=m=$2^{r}$（芝士定义，不是公式，意思是说下面的公式中，r 就是 $log_2(cache 的行数)$）
>
> cache 的容量=$2^{r+w}$ 个字或字节
>
> 标记长度tag=（s-r）位

**优点**：实现简单

**缺点**：较为固定、不够灵活。如果两个对应相同 cache 行的内存块轮流需要被读取，二者就会频繁的把对方从缓存里扔出去，出现「抖动」现象，导致访问速度下降。

适用于大容量 cache

### 全关联映射

**全关联映射允许每一个主存块装入 cache 中的任意行，此时只需要用标记位表示一个主存块。**

> 主存块优先装入 cache 空的行

![image-20211031212059064](https://raw.githubusercontent.com/yijunquan-afk/img-bed-1/main/img2/1695704122.png)

此时，如上图所示，cache 中行的地址编号为：一部分字编码（内存块也有这部分），剩下所有位置都是 tag。

> 在实际实现中，cache 的每一行都额外存储了一个 22bit 的 tag 区域，记录了这行存放的内容在内存块中的块号。

为了确认某一块是否在 cache 中，需要对每一行中的标记进行搜寻检查。**地址中无对应行号的字段**；如果 cache 已满，需要有一个**替换算法**决定丢掉哪行 cache。

![cache](https://telegraph-image-5ms.pages.dev/file/BQACAgUAAyEGAASIfjD1AAM0Z6DZOX9bcTh61A8raNj-J_k51VQAAgoYAAK0lAhVG58atxIKWGU2BA.png)

这张图片说明了使用全关联映射时，CPU 在 cache 中查找指定内容的方式。CPU 获得内容的内存地址，然后去除后几位字地址，留下 tag 区域，和所有 Cache 行存储的 tag 比较，如果能找到就找到了，没找到就没有（真是废话啊）。这个比较过程是**并行的**，所以比较器很不好做（）

<font color="red">**总结如下**</font>

s：地址中块号的长度

w：地址中字号部分的长度。比如一个块有 4 个字，它的 w 就是 2。

> 地址长度=（s+w）位
>
> 可寻址的单元数= $2^{s+w}$个字或字节
>
> 块大小=行大小= $2^{w}$个字或字节
>
> 主存的块数=$2^{s}$
>
> cache 的行数不由地址格式决定
>
> 标记长度 tag=s 位

**优点**：映射灵活，**命中率最高**，不会出现抖动现象

**缺点**：并行比较电路很难设计；tag 位太长（和内存块地址一样长），占用 cache 大小太大了（因为太灵活了）

### 组关联映射

现在的 cache 最经常采用的映射方法，前两种方法的折衷

**主存分成块、cache 分成组**，在组关联映射中，cache 一共有 m 行，其中连续的行分为 v 个组，每组包含 k 个行，它们的关系为：
$$
m=v\times k
$$

> k 行 cache 行被分为组，称为 k 路组关联。所以，k 路组关联指的不是 cache 一共分为四组，而是每组都有四行。

主存块和其映射到的 cache 组的关系为：
$$
i=j\ mode\ v
$$
其中：i 为 cache 组号、j 为主存块号、v 为 cache 的组数。

内存块地址只能确定它被存放到 cache 的哪个组中；具体在组中的哪个行里是随机的。

即块号的低若干位 = 此块进入 cache 时，分配到的 cache 行的组号。这个「若干」 = $log_2(cache 有多少组)$，比如 cache 有 4 组，就需要 2 位作为组号。

> 一般情况下，我们不关心主存的分组，只关心主存的块映射到 cache 的哪个组。
>
> cache 分组是把连续的内容分为一组，比如两两一组时，0、1 分为一组、2、3 分为一组、4、5 分为一组……

映射到同一 cache 组的主存块一定是不连续的（且两两之间相隔一个 cache 大小那么多的块数），这和直接映射一样。

进入 cache 组后，块可以映射到组内的任何一个 cache 行。这是全相联映射思想的体现。

cache 中仍然需要 tag 位来区别这一行的内容来自主存的何处。tag 位的长度就是整个块地址长度 - 字地址长度 - 组地址长度。

![image-20211013101411998](https://raw.githubusercontent.com/yijunquan-afk/img-bed-1/main/img2/1695704123.png)

![image-20211031212735041](https://raw.githubusercontent.com/yijunquan-afk/img-bed-1/main/img2/1695704124.png)

组号的长度 = $log_2(cache 被分为多少组)$，这和直接映射类似。

在组关联映射中，cache 一共有 m 行，其中连续的行分为 v 个组，每组包含 k 个行，它们的关系为：
$$
m=v\times k
$$
当 k=1 时，每组 cache 只包含一行，相当于**直接映射**。

当 v=1 时，cache 整个视为一组，相当于**全相联映射**。

k 一般取 2 的整数次方，取 2 和 4 比较多。

<font color="red">**总结如下**</font>

> 地址长度=（s+w）位
>
> 可寻址的单元数=$2^{s+w}$个字或字节
>
> 块大小=行大小=$2^{w}$个字或字节
>
> 主存的块数=$2^{s}$
>
> cache中每组的行数=k
>
> 组数=$2^d$
>
> cache行数=$m=kv=k\times 2^k$
>
> cache的容量=$k\times 2^k$个字或字节
>
> 标记长度=（s-d）位

**例题**

> 设某机内存容量为 4 MB，Cache 的容量 16 KB ,每块 8 个字,每个字 32 位（4 字节）。设计一个四路组相联映射（即 Cache 内每组包含 4 个块）的 Cache 组织方式，要求：
> 1)画出满足组相联映射的主存地址字段中各字段的位数
> 2)设 Cache 的初态为空，CPU 从主存第 0 号单元开始连续访问 100 个字（主存一次读出一个字）,重复此次序读 8 次,求存储访问的命中率
> 3)若 Cache 的速度是主存速度的 6 倍，求存储系统访问加速比

**解**

> 1）:one: 获得地址长度：地址长度=$\log_2(4MB/4B)=20bits$
>
> > 这里除 4B 而不是 32B 是因为 32 位只是四字节。**一定要注意字节和位的换算关系**。
>
> ​	  :two: 获得cache行数和组数：
>
> ​			字地址长度=$\log_2 8=3bits$
>
> ​			行数=$16KB/(8\times4B)=2^9$，因此行地址长度为 9
>
> ​			对于四路组相联映射，每组有四行，所以组数地址长度= $log_2(2^9/ 4)$ = 7bits
>
> ​			tag地址长度=20-7-3=10bits
>
> ​	  :three: 画出地址表示图如下：
>
> ![image-20211031214826303](https://raw.githubusercontent.com/yijunquan-afk/img-bed-1/main/img2/1695704125.png)
>
> 2）首先要明确一点：<font color="red">只有在第一遍访问时才会存在未命中情况</font>，那么我们就需要计算第一次有多少个字没有命中。
> 当出现一次未命中时，主存就会把对应块上的数据传送到Cache中，那么我们只需要计算出第一遍遍历中主存向Cache传送了多少次数据，就可以得到未命中的次数。100个字需要100/8=13个块。那么第一遍便利的时候主存需要向Cache传送13次数据，也就是说有13次未命中（其余次数内，全部命中了已经加载到 cache 里的行）。全部过程访问8x100=800次，未命中13次，则命中率为(800-13)/800=98.375%
>
> 3）设主存存取周期为6t，那Cache存取周期就为t。
> 加速比就为：6t/(98% xt+2% x7t)=5.3(times)

### 替换算法(考点)

直接映射由于 cache 在的行数确定，无需替换算法（也没法使用替换算法）

全关联映射和组关联映射需要替换算法，以决定缓存已满时，新的待进入缓存的块替换掉缓存中的哪一行。

基于速度的考虑，我们使用**硬件**来实现替换算法

**Least Recently used(LRU)**：选择**最近最少使用**的算法，替换掉那些在 cache 中最长时间未被访问的块。（即寻找存在时间很长且命中次数很低的行）。

实现方法：每行都有一个计数器。当 CPU 命中一行时，此行计数器清零，其他行计数器 +1。替换时，扔掉计数器最大的行。

> 这种方法可以防止刚来的行被直接扔出去（因为累计计数器需要时间），也可以防止扔掉命中次数多的行（命中后计数器清零，重新累积）
>
> LRU 符合局部性原理，因此性能最好。

**First in first out**（FIFO）：先进先出，替换掉在 cache 中停留时间最长的块

> 缺点：和局部性原理一点关系都不沾，没有考虑到使用频繁的块

**Least frequently used**：扔掉最不经常使用的块。即替换掉命中次数最少的块。

> 缺点：新来的行命中次数是 0，因此很有可能刚来就被扔出去，导致白读取了

**Random**：随机选择一行扔出去，性能与LRU几乎差不多。在局部性原理不强的时候，随机算法性能甚至会超过 LRU。

> 纯随机有时候也有点用，不是吗？

![image-20211031215824626](https://raw.githubusercontent.com/yijunquan-afk/img-bed-1/main/img2/1695704127.png)

> 这张图片中，cache 一共包含三行（现实中没有这样的 cache！都是 2^n^ 行的），横向每一格上方表示此时要进入 cache 的内存块地址，下方是这玩意进入 cache 后，cache 的内容。
>
> cache 中，红色表示此次访问命中此行，蓝色表示下一次要被扔出去的行。

### 写策略

与读不同，cache 写更加复杂，因为 cache 是主存的副本，要保证主存、cache 内容的一致性。

当驻留在 cache 中的某块要被替换时，必须考虑两点。如果 cache 中的原块没有被修改过，那么它可以被直接替换掉，而不需要事先写回主存。如果在 cache 某行中至少在一个字上进行过写操作，那么在替换掉该块之前必须将该行**写回主存对应块，以进行主存更新**（否则 CPU 更改的内容就丢失了）。各种可行的写策略都对性能和价格进行了权衡，

#### 写直达(write through)

写操作直接对内存和 cache 进行，保证主存与 cache 数据的一致性。

**缺点**：降低写的速度；产生了大量的存储通信量，可能引起瓶颈；没改变内容的 cache 行也需要被回写。

#### 回写(write back)

写回法(Write-back) ,又称为拷回法(Copy back) ,即**写操作时只把数据写入 Cache 而不写入主存**,但当 Cache 数据**被替换出去**时才写回主存。可见写回法 Cache 中的数据会与主存中的不一致。为了识别 Cache 中的数据是否与主存一致,Cache 中的每一块要增设一个标志位,该位有两个状态:“清"(表示未修改过,与主存一致)和“脏”(表示修改过,与主存不一致)。在 Cache 替换时,“清"的 Cache 块不必写回主存,因为此时主存中相应块的内容与 Cache 块是一致的。在写Cache 时,要将该标志位设置为“浊”,替换时此 Cache 块要写回主存，同时要使标志位为“清”。

适用于迭代运算比较多和 I/O 模块直接连接到 cache 的系统（保证了 CPU 使用的数据最新）

**缺点**：

> 不适用于 I/O 直联主存的系统（cache 的回写可能破坏主存中被 I/O 修改的内容）
>
> 内存中的某些内容无效（因为 cache 更新后的内容暂未写入主存）
>
> 电路很复杂 
>
> cache 可能成为瓶颈

**比较**：写直达法的写操作时间就是访问主存的时间；写回法写操作时间是访问cache的时间，对主存的写操作只发生在块替换时。

**举例**：

> 考虑一个行大小为 32字节的cache和一个传送一个4字节字用时30ns的主存。cache的任意行被替换之前至少已被写过一次，如果要使写回法比写直达法更高效，在被替换之前平均每行被写的次数是多少?
>
> 采用写回法时，每一个脏行只在交换时写回主存一次，需要8 x 30 =240ns。而采用写直达法时，每一次更新cache中的某行都要求有一个字写到主存，耗时30ns。因此，如果行换出之前写入平均超过8次的话，则写回法更有效。

#### 多机系统

保证多 CPU 下 cache 一致性的策略如下：

> 第十七章会提到，多 CPU 下写直达策略用的最多，而且写直达之后还需要通知其他 CPU 将其对应 cache 位置设为无效。

- 基于总线检测的写直达

  每个 CPU 都监视地址线，如果主存某个地址被改写了且此地址内容被缓存到了自己的 cache 中，就设置这行cache 为无效，再次访问时会从主存重新取。

- 硬件透明：

  如果某一个 CPU 修改了自己 cache 中的字，则同时会修改主存对应单元，任何其他 cache 中相同的字也会修改。相当于多 CPU 环境下的写直达。

- 非 cache 存储器

  多个 CPU 共享一段主存，这段内容不可被缓存存储。

### 行大小和命中率的关系

（仍然注意）**局部性原理**：被访问字附近的数据很可能会在不久的将来被访问

行大小：cache 中一行包含的字节数

一般来说，行大小越大，根据局部性原理命中率越高$^1$；但是，行大小太大时，命中率反而会下降。这是因为替换一行的代价太大，而且新数据可能没有旧数据有用。

> $^1$：由于 cache 的读法是「访问主存某个地址时，读取这个地址所在块」。行越大时，内存块越大，读取附近的数据越多，因此命中率越高。

### cache 数量

#### 单 cache 与多级 cache

芯片内 cache(on-chip cache)与芯片外 cache(off-chip cache)：芯片内的 cache 大小会被限制

> 在保持片内 cache 的情况下，人们经过试验，发现添加片外 cache 仍然可以增加访问速度

使用多级 cache：两级 cache：片内 cache 为第一级 L1，外部 cache 为第二级 L2

> 片上缓存L1 (On-chip Cache):到 CPU 的路径短，速度快，降低总线访问频率
>
> 片外缓存L2 (Off-chip Cache):只有 L1 访问缺失才会导致访问 L2

#### unified cache 与 split cache

统一缓存是指指令和数据都存放在同--缓存内的 Cache；分立缓存是指指令和数据分别存放在两个缓存中,一个称为指令 Cache，一个称为数据 Cache。

分立 cache 适用于分布式操作，当采用**超前控制或流水线控制方式**等并行操作时,一般都采用分立缓存；统一 cache 命中率高

两种缓存的选用主要考虑如下两个因素

> 其一,它与主存结构有关,如果计算机的主存是统一的(指令、数据存储在同一主存内),则相应的Cache采用统一缓存;如果主存采用指令、数据分开存储的方案,则相应的Cache采用分立缓存。
>
> 其二,它与机器对指令执行的控制方式有关。当采用**超前控制或流水线控制方式**时,一般都采用分立缓存。

多级缓存中，每级缓存可能采用不同 cache 结构。
